{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNaX/r5VLasCiUcyPhTIRR2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install catboost -q\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5zLSzPhBoP6","executionInfo":{"status":"ok","timestamp":1762446982602,"user_tz":-330,"elapsed":8384,"user":{"displayName":"MCA14_Neeraj","userId":"14803957042055270419"}},"outputId":"797327bf-bb53-43bb-c57a-b56837952153"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["# ===== Win LB: KFold Target Encoding + XGBoost (+CatBoost if available) =====\n","import numpy as np\n","import pandas as pd\n","import xgboost as xgb\n","import warnings\n","from sklearn.model_selection import KFold\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import mean_squared_error\n","warnings.filterwarnings(\"ignore\")\n","\n","# Try CatBoost; continue without if missing (prints same message as before)\n","CAT_OK = True\n","try:\n","    from catboost import CatBoostRegressor, Pool\n","except Exception:\n","    CAT_OK = False\n","    print(\"â„¹ï¸ CatBoost not found (ok). To enable: pip install catboost\")\n","\n","# ---------------- Config ----------------\n","SEED, FOLDS = 42, 5\n","TARGET = \"efficiency\"\n","\n","# ---------------- Load ----------------\n","train = pd.read_csv(\"train.csv\")\n","test  = pd.read_csv(\"test.csv\")\n","sub   = pd.read_csv(\"sample_submission.csv\")\n","\n","feat   = [c for c in train.columns if c != TARGET]\n","X      = train[feat].copy()\n","y      = train[TARGET].copy()\n","X_test = test[feat].copy()\n","\n","num_cols = X.select_dtypes(include=\"number\").columns.tolist()\n","cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n","\n","# ---------------- Clean ----------------\n","# Numeric impute (fit on train, apply to test)\n","num_imp = SimpleImputer(strategy=\"median\")\n","X[num_cols]      = num_imp.fit_transform(X[num_cols])\n","X_test[num_cols] = num_imp.transform(X_test[num_cols])\n","\n","# Categorical NA to \"NA\"\n","for c in cat_cols:\n","    X[c]      = X[c].astype(str).fillna(\"NA\")\n","    X_test[c] = X_test[c].astype(str).fillna(\"NA\")\n","\n","# Rare merge (helps generalization) â€” same logic\n","def merge_rare(s: pd.Series, min_count=20):\n","    vc = s.value_counts()\n","    return s.where(s.map(vc) >= min_count, \"RARE\")\n","\n","for c in cat_cols:\n","    X[c] = merge_rare(X[c])\n","    seen = set(X[c].unique())\n","    X_test.loc[~X_test[c].isin(seen), c] = \"RARE\"\n","\n","# Target bounds (used for clipping at the end)\n","y_min, y_max = float(y.min()), float(y.max())\n","\n","# ---------------- Optional log1p target ----------------\n","USE_LOG = (y > 0).all()\n","if USE_LOG:\n","    y_train_fit = np.log1p(y)\n","    print(\"ðŸ” Using log1p target.\")\n","else:\n","    y_train_fit = y.values\n","\n","# ---------------- KFold Target Encoding ----------------\n","# Per-fold mean encoding with smoothing; applied to train folds and test using full-train means.\n","def kfold_target_encode(train_df, test_df, y_vec, cats, n_splits=FOLDS, seed=SEED, smoothing=20.0):\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    oof_te  = pd.DataFrame(index=train_df.index)\n","    test_te = pd.DataFrame(index=test_df.index)\n","    global_mean = y_vec.mean()\n","\n","    for c in cats:\n","        oof_col  = np.zeros(len(train_df))\n","        test_col = np.zeros(len(test_df))\n","\n","        for tr_idx, vl_idx in kf.split(train_df):\n","            tr, vl = train_df.iloc[tr_idx], train_df.iloc[vl_idx]\n","            y_tr   = y_vec.iloc[tr_idx] if isinstance(y_vec, pd.Series) else y_vec[tr_idx]\n","\n","            counts = tr[c].value_counts()\n","            means  = tr.join(pd.Series(y_tr, index=tr.index).rename(\"__y\")).groupby(c)[\"__y\"].mean()\n","\n","            # smoothing: (count*mean + s*global) / (count + s)\n","            denom   = counts.reindex(means.index).fillna(0.0)\n","            sm_mean = (means * denom + smoothing * global_mean) / (denom + smoothing)\n","\n","            oof_col[vl_idx] = vl[c].map(sm_mean).fillna(global_mean).values\n","\n","        # test encoding: fit on full train\n","        counts_full = train_df[c].value_counts()\n","        means_full  = train_df.join(pd.Series(y_vec, index=train_df.index).rename(\"__y\")).groupby(c)[\"__y\"].mean()\n","        denom_full  = counts_full.reindex(means_full.index).fillna(0.0)\n","        sm_full     = (means_full * denom_full + smoothing * global_mean) / (denom_full + smoothing)\n","\n","        test_col = test_df[c].map(sm_full).fillna(global_mean).values\n","\n","        oof_te[f\"TE__{c}\"]  = oof_col\n","        test_te[f\"TE__{c}\"] = test_col\n","\n","    return oof_te, test_te\n","\n","# Build TE features (use the same target form we train on)\n","y_for_te = pd.Series(y_train_fit, index=X.index) if USE_LOG else pd.Series(y, index=X.index)\n","TE_tr, TE_te = kfold_target_encode(X, X_test, y_for_te, cat_cols)\n","\n","# Final matrices for XGB: [numerics + TE] (no OHE, same as original)\n","X_xgb     = np.hstack([X[num_cols].values,      TE_tr.values])\n","Xtest_xgb = np.hstack([X_test[num_cols].values, TE_te.values])\n","\n","# ---------------- XGBoost (5-fold CV) ----------------\n","params = dict(\n","    objective=\"reg:squarederror\",\n","    learning_rate=0.03,\n","    max_depth=7,\n","    min_child_weight=8,\n","    subsample=0.85,\n","    colsample_bytree=0.85,\n","    reg_lambda=3.0,\n","    reg_alpha=0.1\n",")\n","\n","kf = KFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n","oof_xgb    = np.zeros(len(X))\n","pred_xgb   = np.zeros(len(X_test))\n","best_rounds = []\n","\n","for fold, (tr_idx, vl_idx) in enumerate(kf.split(X_xgb), 1):\n","    dtr = xgb.DMatrix(X_xgb[tr_idx], label=(y_train_fit[tr_idx] if USE_LOG else y.iloc[tr_idx]))\n","    dvl = xgb.DMatrix(X_xgb[vl_idx],  label=(y_train_fit[vl_idx] if USE_LOG else y.iloc[vl_idx]))\n","\n","    m = xgb.train(params, dtr, num_boost_round=8000,\n","                  evals=[(dvl, \"val\")], early_stopping_rounds=400, verbose_eval=False)\n","\n","    best_rounds.append(m.best_iteration + 1)\n","    oof_xgb[vl_idx] = m.predict(dvl)\n","\n","    # Fold-wise model on full data for test contribution (same logic)\n","    pred_xgb += xgb.train(params, xgb.DMatrix(X_xgb, label=(y_train_fit if USE_LOG else y)),\n","                          num_boost_round=m.best_iteration + 1, verbose_eval=False) \\\n","                          .predict(xgb.DMatrix(Xtest_xgb)) / FOLDS\n","\n","# Undo log if used (same function/behavior)\n","def inv_t(z):\n","    return np.expm1(z) if USE_LOG else z\n","\n","oof_xgb_real  = inv_t(oof_xgb)\n","pred_xgb_real = inv_t(pred_xgb)\n","\n","rmse_xgb = mean_squared_error(y, oof_xgb_real) ** 0.5\n","print(f\"âœ… XGB OOF RMSE: {rmse_xgb:.6f} | mean trees: {np.mean(best_rounds):.0f}\")\n","\n","# ---------------- CatBoost (optional, if installed) ----------------\n","use_cat = CAT_OK and (len(cat_cols) > 0)\n","if use_cat:\n","    oof_cat  = np.zeros(len(X))\n","    pred_cat = np.zeros(len(X_test))\n","\n","    params_cat = dict(\n","        loss_function=\"RMSE\",\n","        learning_rate=0.03,\n","        depth=8,\n","        l2_leaf_reg=6.0,\n","        iterations=20000,\n","        od_type=\"Iter\",\n","        od_wait=400,\n","        random_seed=SEED,\n","        verbose=False\n","    )\n","\n","    for tr_idx, vl_idx in kf.split(X):\n","        tr_pool = Pool(\n","            X.iloc[tr_idx],\n","            (np.log1p(y.iloc[tr_idx]) if USE_LOG else y.iloc[tr_idx]),\n","            cat_features=[X.columns.get_loc(c) for c in cat_cols]\n","        )\n","        vl_pool = Pool(\n","            X.iloc[vl_idx],\n","            (np.log1p(y.iloc[vl_idx]) if USE_LOG else y.iloc[vl_idx]),\n","            cat_features=[X.columns.get_loc(c) for c in cat_cols]\n","        )\n","\n","        m = CatBoostRegressor(**params_cat)\n","        m.fit(tr_pool, eval_set=vl_pool, use_best_model=True)\n","\n","        oof_cat[vl_idx]  = m.predict(vl_pool)\n","        pred_cat        += m.predict(Pool(X_test, cat_features=[X.columns.get_loc(c) for c in cat_cols])) / FOLDS\n","\n","    oof_cat_real  = inv_t(oof_cat)\n","    pred_cat_real = inv_t(pred_cat)\n","    rmse_cat = mean_squared_error(y, oof_cat_real) ** 0.5\n","    print(f\"âœ… CAT OOF RMSE: {rmse_cat:.6f}\")\n","\n","# ---------------- OOF-based blending ----------------\n","if use_cat:\n","    alphas = np.linspace(0, 1, 51)  # weight on XGB\n","    best_rmse, best_a = 1e9, None\n","    for a in alphas:\n","        blend_oof = a * oof_xgb_real + (1 - a) * oof_cat_real\n","        r = mean_squared_error(y, blend_oof) ** 0.5\n","        if r < best_rmse:\n","            best_rmse, best_a = r, a\n","    print(f\"ðŸ¥‡ Blend OOF RMSE: {best_rmse:.6f} (alpha={best_a:.2f} â†’ XGB weight)\")\n","    final_pred = best_a * pred_xgb_real + (1 - best_a) * pred_cat_real\n","else:\n","    final_pred = pred_xgb_real\n","\n","# ---------------- Clip + Submit ----------------\n","# If your target is truly [0,1], set these explicitly to (0.0, 1.0)\n","final_pred = np.clip(final_pred, y_min, y_max)\n","\n","if \"efficiency\" not in sub.columns:\n","    raise ValueError(\"sample_submission.csv needs 'efficiency' column.\")\n","if len(sub) != len(final_pred):\n","    raise ValueError(\"Row mismatch between sample_submission and predictions.\")\n","\n","sub[\"efficiency\"] = final_pred\n","sub.to_csv(\"submission.csv\", index=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QD3U_Jy_BuPS","executionInfo":{"status":"ok","timestamp":1762449081845,"user_tz":-330,"elapsed":133865,"user":{"displayName":"MCA14_Neeraj","userId":"14803957042055270419"}},"outputId":"862a9df2-a761-4249-bb9d-2abfb929333f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ” Using log1p target.\n","âœ… XGB OOF RMSE: 0.881561 | mean trees: 301\n","âœ… CAT OOF RMSE: 0.787303\n","ðŸ¥‡ Blend OOF RMSE: 0.787303 (alpha=0.00 â†’ XGB weight)\n","ðŸ“„ Saved submission.csv\n","   id  efficiency\n","0   1    8.304069\n","1   4   10.549170\n","2   5   12.320598\n","3   9    5.000000\n","4  11   14.681001\n"]}]}]}